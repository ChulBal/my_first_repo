{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learning rate\n",
    "## To find better learning rate, visualize how cost function decreases.\n",
    "## 1.1 With small learning rate : 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[1, 2, 1],\n",
    "          [1, 3, 2],\n",
    "          [1, 3, 4],\n",
    "          [1, 5, 5],\n",
    "          [1, 7, 5],\n",
    "          [1, 2, 5],\n",
    "          [1, 6, 6],\n",
    "          [1, 7, 7]])\n",
    "\n",
    "y_data = np.array([[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]])\n",
    "\n",
    "x_test = np.array([[2, 1, 1], [3, 1, 2], [3, 3, 4]])\n",
    "\n",
    "y_test = np.array([[0, 0, 1], [0, 0, 1], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, 3])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([3, 3]))\n",
    "b = tf.Variable(tf.random_normal([3]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Cost'):\n",
    "    cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "    tf.summary.scalar(\"CrossEntropy\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10)\n",
    "    train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = './tensorboard-log/log-small_lr'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR, sess.graph)\n",
    "    merged= tf.summary.merge_all()\n",
    "        \n",
    "    for step in range(5000):\n",
    "        _, summary = sess.run([train, merged], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 With large learning rate : 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5)\n",
    "    train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = './tensorboard-log/log-large_lr'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR, sess.graph)\n",
    "    merged= tf.summary.merge_all()\n",
    "        \n",
    "    for step in range(5000):\n",
    "        _, summary = sess.run([train, merged], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 With proper learning rate : 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('Optimizer'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    train = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGDIR = './tensorboard-log/log-lr'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR, sess.graph)\n",
    "    merged= tf.summary.merge_all()\n",
    "        \n",
    "    for step in range(5000):\n",
    "        _, summary = sess.run([train, merged], feed_dict={X: x_data, Y: y_data})\n",
    "        writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard --logdir=./tensorboard-log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![cross entropy](https://github.com/ChulBal/practice_makes_perfect/blob/master/cross.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'Why no graph with large learning rate?' \n",
    "### Small learning rate? There are outputs although it does not decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "500 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "1000 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "1500 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "2000 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "2500 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "3000 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "3500 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "4000 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n",
      "4500 5.73203 [[ 0.80269563  0.67861295 -1.21728313]\n",
      " [-0.3051686  -0.3032113   1.50825703]\n",
      " [ 0.75722361 -0.7008909  -2.10820389]]\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-10).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(5000):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(step, cost_val, W_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big learning rate? There are 'nan's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.73203 [[-0.30548954  1.22985029 -0.66033536]\n",
      " [-4.39069986  2.29670858  2.99386835]\n",
      " [-3.34510708  2.09743214 -0.80419564]]\n",
      "500 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "1500 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "2000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "2500 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "3000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "3500 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "4000 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n",
      "4500 nan [[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]]\n"
     ]
    }
   ],
   "source": [
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.5).minimize(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(5000):\n",
    "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
    "        if step % 500 == 0:\n",
    "            print(step, cost_val, W_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scaling\n",
    "## 2.1 Min-Max Scaler\n",
    "### step 1000, learning rate 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cost:  2.02763e+11\n",
      "100 Cost:  nan\n",
      "200 Cost:  nan\n",
      "300 Cost:  nan\n",
      "400 Cost:  nan\n",
      "500 Cost:  nan\n",
      "600 Cost:  nan\n",
      "700 Cost:  nan\n",
      "800 Cost:  nan\n",
      "900 Cost:  nan\n",
      "1000 Cost:  nan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "x_data1 = xy[:, 0:-1]\n",
    "y_data1 = xy[:, [-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1001):\n",
    "    cost_val, _, _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data1, Y: y_data1})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99999999  0.99999999  0.          1.          1.        ]\n",
      " [ 0.70548491  0.70439552  1.          0.71881782  0.83755791]\n",
      " [ 0.54412549  0.50274824  0.57608696  0.606468    0.6606331 ]\n",
      " [ 0.33890353  0.31368023  0.10869565  0.45989134  0.43800918]\n",
      " [ 0.51436     0.42582389  0.30434783  0.58504805  0.42624401]\n",
      " [ 0.49556179  0.42582389  0.31521739  0.48131134  0.49276137]\n",
      " [ 0.11436064  0.          0.20652174  0.22007776  0.18597238]\n",
      " [ 0.          0.07747099  0.5326087   0.          0.        ]]\n",
      "0 Cost:  0.403202\n",
      "100 Cost:  0.00409762\n",
      "200 Cost:  0.00310639\n",
      "300 Cost:  0.00303583\n",
      "400 Cost:  0.0029996\n",
      "500 Cost:  0.00296937\n",
      "600 Cost:  0.00294309\n",
      "700 Cost:  0.00292018\n",
      "800 Cost:  0.00290022\n",
      "900 Cost:  0.00288281\n",
      "1000 Cost:  0.00286762\n"
     ]
    }
   ],
   "source": [
    "def MinMaxScaler(data):\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    return numerator / (denominator + 1e-7)\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "\n",
    "xy2 = MinMaxScaler(xy)\n",
    "x_data2 = xy2[:, 0:-1]\n",
    "y_data2 = xy2[:, [-1]]\n",
    "\n",
    "print(xy2)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate= 0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1001):\n",
    "    cost_val, _ , _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data2, Y: y_data2})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"StandardGD\", cost)\n",
    "    \n",
    "LOGDIR = './tensorboard_scaling/min_max'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR, sess.graph)\n",
    "    merged= tf.summary.merge_all()\n",
    "        \n",
    "    for step in range(5000):\n",
    "        _, summary = sess.run([train, merged], feed_dict={X: x_data2, Y: y_data2})\n",
    "        writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.80044859  1.88513361 -1.28230078  1.72787156  1.61889903]\n",
      " [ 0.81097514  0.9053588   2.08831842  0.73846426  1.08747361]\n",
      " [ 0.26886075  0.23700312  0.65946897  0.34313432  0.50866828]\n",
      " [-0.42061729 -0.38965881 -0.91592913 -0.17263101 -0.21964058]\n",
      " [ 0.16885854 -0.017961   -0.25646016  0.26776306 -0.25813006]\n",
      " [ 0.10570276 -0.017961   -0.21982299 -0.0972596  -0.04052009]\n",
      " [-1.17500725 -1.42934534 -0.58619464 -1.01647295 -1.04417298]\n",
      " [-1.55922124 -1.17256937  0.51292031 -1.79086963 -1.65257721]]\n",
      "0 Cost:  4.83767\n",
      "100 Cost:  0.0297934\n",
      "200 Cost:  0.0295568\n",
      "300 Cost:  0.0295013\n",
      "400 Cost:  0.0294827\n",
      "500 Cost:  0.0294725\n",
      "600 Cost:  0.0294651\n",
      "700 Cost:  0.0294591\n",
      "800 Cost:  0.0294541\n",
      "900 Cost:  0.02945\n",
      "1000 Cost:  0.0294465\n"
     ]
    }
   ],
   "source": [
    "def Standardization(data):\n",
    "    numerator = data - np.mean(data, 0)\n",
    "    denominator = np.std(data, 0)\n",
    "    return numerator / denominator\n",
    "\n",
    "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
    "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
    "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
    "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
    "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
    "               [819, 823, 1198100, 816, 820.450012],\n",
    "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
    "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
    "\n",
    "xy3 = Standardization(xy)\n",
    "print(xy3)\n",
    "\n",
    "x_data3 = xy3[:, 0:-1]\n",
    "y_data3 = xy3[:, [-1]]\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate= 0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(1001):\n",
    "    cost_val, _ , _ = sess.run([cost, hypothesis, train], feed_dict={X: x_data3, Y: y_data3})\n",
    "    if step % 100 == 0:\n",
    "        print(step, \"Cost: \", cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"StandardGD\", cost)\n",
    "    \n",
    "LOGDIR = './tensorboard_scaling/standardization'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(LOGDIR, sess.graph)\n",
    "    merged= tf.summary.merge_all()\n",
    "        \n",
    "    for step in range(5000):\n",
    "        _, summary = sess.run([train, merged], feed_dict={X: x_data3, Y: y_data3})\n",
    "        writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ![scaling](https://github.com/ChulBal/practice_makes_perfect/blob/master/scaling.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Batch vs Stochastic vs Minibatch?\n",
    "## 3.1 MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 2.894389667\n",
      "Epoch: 0002 cost = 1.080790193\n",
      "Epoch: 0003 cost = 0.865806107\n",
      "Epoch: 0004 cost = 0.760368438\n",
      "Epoch: 0005 cost = 0.693575912\n",
      "Epoch: 0006 cost = 0.646156837\n",
      "Epoch: 0007 cost = 0.609966849\n",
      "Epoch: 0008 cost = 0.580936522\n",
      "Epoch: 0009 cost = 0.556751002\n",
      "Epoch: 0010 cost = 0.536568008\n",
      "Epoch: 0011 cost = 0.518954005\n",
      "Epoch: 0012 cost = 0.503147653\n",
      "Epoch: 0013 cost = 0.489707803\n",
      "Epoch: 0014 cost = 0.477875151\n",
      "Epoch: 0015 cost = 0.466704100\n",
      "Learning finished\n",
      "Accuracy:  0.891\n",
      "Label:  [0]\n",
      "Prediction:  [0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPxAU4QeCIvxAUIQfCIrwA0H9P7PdGv1f511HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c847fd400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(777) \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "    print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "    plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Online Learning vs Batch Learning?\n",
    "### Online Learning is generally described as doing machine learning in a streaming data setting, i.e. training a model in conscecutive rounds. Whereas in batch learning you have access to the whole dataset on train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![batch learning](https://github.com/ChulBal/practice_makes_perfect/blob/master/batch_learning.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![online learning](https://github.com/ChulBal/practice_makes_perfect/blob/master/online_learning.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1 Stochastic Gradient Descent\n",
    "## What is Stochastic Gradient Descent?\n",
    "## 1) Batch gradient descent\n",
    "# ![Standard_GD](https://github.com/ChulBal/practice_makes_perfect/blob/master/Standard_GD.png?raw=true)\n",
    "## 2) Stochastic gradient descent\n",
    "###### ![stochastic cost](https://github.com/ChulBal/practice_makes_perfect/blob/master/Stochastic_GD_cost.png?raw=true)\n",
    "###### ![stochastic](https://github.com/ChulBal/practice_makes_perfect/blob/master/Stochastic_GD.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kaggle Stochastic Gradient Descent\n",
    "https://www.kaggle.com/kartikmehtaiitd/stochastic-gradient-descent-0-77355/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reference\n",
    "##### http://ruder.io/optimizing-gradient-descent/index.html#stochasticgradientdescent\n",
    "##### http://adventuresinmachinelearning.com/stochastic-gradient-descent/\n",
    "##### https://www.slideshare.net/queirozfcom/online-machine-learning-introduction-and-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Stochastic Gradient Descent with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "x, y = mnist[\"data\"], mnist[\"target\"]\n",
    "import numpy as np\n",
    "x_train, x_test, y_train, y_test = x[:60000], x[60000:], y[:60000], y[60000:]\n",
    "shuffle_index = np.random.permutation(60000)\n",
    "x_train, y_train = x_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state=42,loss = 'log', max_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.88217357,  0.86174309,  0.86888033])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, x_train, y_train, cv = 3, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.90841832,  0.91089554,  0.91008651])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, x_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "#### http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "#### https://github.com/ageron/handson-ml/blob/master/03_classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Minibatch learning\n",
    "### Use b examples in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.set_random_seed(777)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "nb_classes = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 14.030431747\n",
      "cost = 5.508152962\n",
      "cost = 3.627789259\n",
      "cost = 2.575377226\n",
      "cost = 2.651850224\n",
      "cost = 1.842662334\n",
      "cost = 1.736264229\n",
      "cost = 1.845879078\n",
      "cost = 1.490944624\n",
      "cost = 1.063835740\n",
      "cost = 0.921803713\n",
      "Learning finished\n",
      "Accuracy:  0.7539\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    batch_size = 100\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        if i % 50 == 0:\n",
    "            print('cost =', '{:.9f}'.format(c))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 14.425215721\n",
      "cost = 9.155404091\n",
      "cost = 5.874150276\n",
      "cost = 4.483073711\n",
      "cost = 3.437642097\n",
      "cost = 3.344949007\n",
      "cost = 3.337590218\n",
      "cost = 2.514959812\n",
      "cost = 2.446213007\n",
      "cost = 1.755002737\n",
      "cost = 2.269384146\n",
      "Learning finished\n",
      "Accuracy:  0.6461\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    batch_size = 200\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        if i % 25 == 0:\n",
    "            print('cost =', '{:.9f}'.format(c))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 11.438349724\n",
      "cost = 0.801296830\n",
      "cost = 0.737648845\n",
      "cost = 1.208081841\n",
      "cost = 1.267851114\n",
      "cost = 2.895649672\n",
      "cost = 0.813413143\n",
      "cost = 0.978428185\n",
      "cost = 0.028947284\n",
      "cost = 0.006509293\n",
      "cost = 2.446244955\n",
      "Learning finished\n",
      "Accuracy:  0.8833\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    batch_size = 10    \n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        if i % 500 == 0:\n",
    "            print('cost =', '{:.9f}'.format(c))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "    print(\"Accuracy: \", accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "#### http://adventuresinmachinelearning.com/stochastic-gradient-descent/\n",
    "#### https://www.quora.com/Intuitively-how-does-mini-batch-size-affect-the-performance-of-stochastic-gradient-descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
